name: "128"
layers:
  - _target_: keras.layers.Flatten
  - _target_: larq.layers.QuantDense
    units: 128
    input_quantizer: ste_sign
    kernel_quantizer: ste_sign
    kernel_constraint: weight_clip
  - _target_: larq.layers.QuantDense
    units: 10
    activation: softmax
    input_quantizer: ste_sign
    kernel_quantizer: ste_sign
    kernel_constraint: weight_clip
compile:
  optimizer:
    _target_: keras.optimizers.Adam
    learning_rate: 0.0005
  loss: categorical_crossentropy
  metrics:
    - accuracy
fit:
  epochs: 50
  batch_size: 128
